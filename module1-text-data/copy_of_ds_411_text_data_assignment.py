# -*- coding: utf-8 -*-
"""Copy of DS_411_Text_Data_Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11bszlm1PydlmodLOeCCahejbBM9fDL4j

# Natural Language Processing (NLP)
## *Data Science Unit 4 Sprint 1 Assignment 1*

Your goal in this assignment: find the attributes of the best & worst coffee shops in the dataset. The text is fairly raw: dates in the review, extra words in the `star_rating` column, etc. You'll probably want to clean that stuff up for a better analysis.

Analyze the corpus of text using text visualizations of token frequency. Try cleaning the data as much as possible. Try the following techniques:
- Lemmatization
- Custom stopword removal

Keep in mind the attributes of good tokens. Once you have a solid baseline, layer in the star rating in your visualization(s). Key part of this assignment - produce a write-up of the attributes of the best and worst coffee shops. Based on your analysis, what makes the best the best and the worst the worst. Use graphs and numbesr from your analysis to support your conclusions. There should be plenty of markdown cells! :coffee:
"""

from IPython.display import YouTubeVideo
YouTubeVideo('Jml7NVYm8cs')

"""#0. Prepare

## 0.1 Get spacy
"""

# Locally (or on colab) let's use en_core_web_lg
#!python -m spacy download en_core_web_md # Can do lg, takes awhile
# Also on Colab, need to restart runtime after this step!

"""## 0.2 Restart runtime!

## 0.3 Install packages and do imports
"""

!pip install squarify

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

"""-----

## 0.4 Get data
"""

url = "https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-1-NLP/main/module1-text-data/data/yelp_coffeeshop_review_data.csv"

df = pd.read_csv(url)
df.head()

df['full_review_text'][0]

"""# 1 Practice: Clean, tokenize, remove stop words, lemmatize

## 1.0 Save Dates for later use (optional)
Each Review starts with a date, we could grab this date and add it as a new column on our dataframe if we want. This could be a valuable feature later on. This isn't absolutely necessary, but might be a good idea. Otherwise we might just be throwing away this useful information when we clean the text.
"""

# grab date from the beginning of the review text
def get_date(text):
  return text.split(' ')[1]

df['date'] = df['full_review_text'].apply(get_date)

df

# Another approach (we could also use regular expressions to grab the dates)
import re

def find_first_date(text):
  #return re.findall('\d+\/\d+\/\d+', text)[0]
  return re.findall(r'\d+/\d+/\d+', text)[0]

example_text = " 11/25/2016 1 check-in Love love loved the atmosphere! Every corner of the coffee shop had its own style, and there were swings!!! I ordered the matcha latte, and it was muy fantastico! Ordering and getting my drink were pretty streamlined. I ordered on an iPad, which included all beverage selections that ranged from coffee to wine, desired level of sweetness, and a checkout system. I got my latte within minutes!  I was hoping for a typical heart or feather on my latte, but found myself listing out all the possibilities of what the art may be. Any ideas? "

find_first_date(example_text)

print(df['date'].iloc[0])
print(type(df['date'].iloc[0]))

# turn date column strings into datetime objects
df['date'] = pd.to_datetime(df['date'], infer_datetime_format=True)

print(type(df['date'][0]))
print(df['date'].iloc[0])

dir(df['date'].iloc[0])

df['date'].iloc[0].day_name()

df.head()

df['star_rating'].unique()

df['star_rating'].value_counts()

# looks like most dates are in late 2016
df['date'].value_counts()

# histogram of review dates to see when most reviews were left
# just a little EDA
df['date'].hist(bins=20);

"""## 1.1 Clean Review Text (with Regular Expressions `regex`)
dataquest has a good [regex reference](https://www.dataquest.io/wp-content/uploads/2019/03/python-regular-expressions-cheat-sheet.pdf
)

It looks like the reviews have the following characteristics

- Date at the beginning
- Lots of punctuation
- Use special characters like $ (as well as dollar amounts)
- Upper and lower case words
"""

import re
# considering using regex to remove dates
# you can create and test regular expressions on this online regex editor: https://regex101.com/

def clean_data(text):
    """
    Accepts a single text document and performs several regex substitutions in order to clean the document.

    Parameters
    ----------
    text: string or object

    Returns
    -------
    text: string or object
    """

    # order of operations - apply the expression from top to bottom
    date_regex = r"\d+/\d+/\d+"
    punct_regex = r"[^0-9a-zA-Z\s]" # any non-alphanumeric chars
    special_chars_regex = r"[\$\%\&\@+]"
    numerical_regex =  r"\d+"  # match one or more digits

    # Replace any strings matching the above regex patterns with blank strings
    # (effectively removing them from the text)
    text = re.sub(date_regex, "", text)
    text = re.sub(punct_regex, "", text)
    text = re.sub(special_chars_regex, "", text)
    text = re.sub(numerical_regex, "", text)

    # match one or more whitespace chars
    whitespace_regex = r"\s+"
    # replace one or more whitespace characters with a single white space char
    #    not a blank string!
    text = re.sub(whitespace_regex, " ", text)

    # COnvert the text to lowercase
    text = text.lower()

    return text

df['cleaned_review'] = df['full_review_text'].apply(clean_data)

df['cleaned_review'][0]

"""## 1.2. Tokenize the cleaned up reviews"""

# with a for loop
tokens = []
for review in df['cleaned_review']:
  tokens.append(review.split(' '))

df['tokens'] = tokens

df.head()

df['coffee_shop_name'].value_counts()
len(df[df['coffee_shop_name'] == 'Summer Moon Coffee Bar'])
len(df[df['coffee_shop_name'].str.contains('Summer Moon Coffee Bar', case=False, na=False)])

filtered_df = df[df['coffee_shop_name'].str.contains('Summer Moon Coffee Bar', case=False, na=False)]
filtered_df

df.info()

# with a list comprehension
df['tokens'] = df['cleaned_review'].apply(lambda x: x.split())
df.head()

"""### 1.3 Visualize the most common tokens
Below is the `count()` function that we used in the lecture.
"""

from collections import Counter

def count(token_lists):
    """
    Calculates some basic statistics about tokens in our corpus (i.e. corpus means collections text data)
    """
    # stores the count of each token
    word_counts = Counter()

    # stores the number of docs that each token appears in
    appears_in_docs = Counter()

    total_docs = len(token_lists)

    for token_list in token_lists:
        # stores count of every appearance of a token
        word_counts.update(token_list)

        # use set() in order to not count duplicates, thereby count the num of docs that each token appears in
        appears_in_docs.update(set(token_list))

    # build word count dataframe
    word_count_dict = zip(word_counts.keys(), word_counts.values())
    wc = pd.DataFrame(word_count_dict, columns = ['word', 'count'])

    # rank the the word counts
    wc['rank'] = wc['count'].rank(method='first', ascending=False)
    total = wc['count'].sum()

    # calculate the percent total of each token
    wc['fraction_of_total'] = wc['count'].apply(lambda token_count: token_count / total)

    # calculate the cumulative percent total of word counts
    wc = wc.sort_values(by='rank')
    wc['cumulative_fraction_of_total'] = wc['fraction_of_total'].cumsum()

    # create dataframe for document stats
    t2 = zip(appears_in_docs.keys(), appears_in_docs.values())
    ac = pd.DataFrame(t2, columns=['word', 'appears_in_docs'])

    # merge word count stats with doc stats
    wc = ac.merge(wc, on='word')

    wc['appears_in_fraction_of_docs'] = wc['appears_in_docs'].apply(lambda x: x / total_docs)

    return wc.sort_values(by='rank')

# Use the function to count tokens in the 'tokens' column
token_counts = count('tokens')
token_counts

"""#### Summary of the descriptive token statistics

`word` The specific token that is being analyzed

`appears_in_docs` Number of documents that the word/token appears in

`count` The total number of appearances of that token within the corpus

`rank` Ranking of tokens by count

`fraction_of_total` Fraction of the total tokens that this token makes up

`cumulative_fraction_of_total` Sum of fractional total of ranked tokens, down to and including this token.

`appears_in_fraction_of_docs` Fraction of documents that token appears in
"""

try:
    # Define the text cleaning function
    def clean_data(text):
        # Regular expressions for date, punctuation, special characters, and numerical values
        date_regex = r"\d+/\d+/\d+"
        punct_regex = r"[^0-9a-zA-Z\s]"
        special_chars_regex = r"[\$\%\&\@+]"
        numerical_regex = r"\d+"

        # Remove the matching strings
        text = re.sub(date_regex, "", text)
        text = re.sub(punct_regex, "", text)
        text = re.sub(special_chars_regex, "", text)
        text = re.sub(numerical_regex, "", text)

        # Replace one or more whitespace characters with a single whitespace
        whitespace_regex = r"\s+"
        text = re.sub(whitespace_regex, " ", text)

        # Convert the text to lowercase
        text = text.lower()

        return text

    # Define the token counting function
    def count_tokens(df, column):
        # Initialize a Counter object from the collections module
        word_counts = Counter()

        # Iterate through each row of tokenized text and update the word counts
        for _, row in df.iterrows():
            word_counts.update(row[column])

        return word_counts

    # Load the csv file into a pandas DataFrame
    df = pd.read_csv(url)

    # Apply the cleaning function to the full_review_text column
    df['cleaned_review'] = df['full_review_text'].apply(clean_data)

    # Tokenize the cleaned reviews
    df['tokens'] = df['cleaned_review'].apply(lambda x: x.split())

    # Use the function to count tokens in the 'tokens' column
    token_counts = count_tokens(df, 'tokens')

except Exception as e:
    print("An error occurred:", str(e))

try:
    # Convert the Counter object to a DataFrame
    tokens_df = pd.DataFrame.from_dict(token_counts, orient='index', columns=['count'])

    # Sort the DataFrame by count in descending order and print the top 20 words
    tokens_df = tokens_df.sort_values('count', ascending=False)
    tokens_df.head(20).plot(kind='barh', figsize=(5,3.5))
    plt.xlabel("Count")
    plt.ylabel("Token")
    plt.title("Top 20 Tokens")
    plt.gca().invert_yaxis()
    plt.show()

except Exception as e:
    print("An error occurred:", str(e))

# visualize the 20 most common tokens with squarify
# Lots of stopwords!
import squarify

#YOUR CODE HERE
wc = tokens_df.head(20)

# Create a treemap with squarify
fig = plt.gcf()
ax = fig.add_subplot()
fig.set_size_inches(8, 3)
squarify.plot(sizes=wc['count'],
              label=wc.index,
              alpha=0.6)

plt.title("Top 20 Tokens",fontsize=23,fontweight="bold")
plt.axis('off')
plt.show()

#squarify.plot(sizes=, label=, alpha=.8 )
#
# plt.axis('off')
# plt.show()

"""## 1.4 Clean, tokenize, remove stopwords, and lemmatize the reviews (with Spacy)

We can do all of the above with Spacy, but Spacy has the added benefit of making it easy to lemmatize tokens and remove stop words as well! Let's write a tokenize function with Spacy.
"""

import spacy
# import the large version of the pre-trained model
# depending on your computational resources/limitations, you might need to download and load in a smaller version of the model
# see the spaCy docs: https://spacy.io/models/en
nlp = spacy.load('en_core_web_sm')

"""## Remove stop word, punctuation and whitespace from each token"""

# this will take longer to run than a regex method because Spacy is doing a fair amount
# of extra stuff under the hood in creating the token attributes and everything.

def spacy_tokenizer(text):
    """
    Use the pre-trained model from Space to tokenize our text into lemmas

    Notes
    -----
    Rememeber that the pre-trained spaCy model has a lot of built in flags for what kind of token each token is
    so we can use that functionality to create filters for stop words, white spaces, punctuation, and so on!

    See list of flags here: https://spacy.io/api/token#attributes

    Parameter
    ---------
    text: string
        Full text article/document that needs to be tokenized
    """
    tokens = []
    for token in nlp(text):
        # if statement will filter out stopwords, punctuation, and whitespace
        if not token.is_stop and not token.is_punct and not token.is_space:
            # Now lemmatize!
            tokens.append(token.lemma_)

    return tokens

df['full_review_text'][10]

# Commented out IPython magic to ensure Python compatibility.
# # This code cell takes ~ 3 minutes on Google Colab
# %%time
# # YOUR CODE HERE
# df['spacy_tokens'] = df['full_review_text'].apply(spacy_tokenizer)

df.head()

"""##1.5 Visualize the Spacy Tokens"""

wc = count(df['spacy_tokens'])

wc_top20 = wc[wc['rank'] <= 20]

squarify.plot(sizes=wc_top20['fraction_of_total'], label=wc_top20['word'], alpha=0.6)

plt.axis('off')
plt.show()

"""# 2. Compare High and Low Coffeeshop reviews!

- We'll say that a "good" rating is 4 or 5 stars
- a "bad" rating is 3 stars or less

## 2.1 Create a numeric version of the star rating
"""

df.head()

df['star_rating'].unique()

df.star_rating.iloc[:3]

"""Let's write a `regex` to extract only the numerical value of the rating from this text string<br>
Reference: [Easiest way to remember Regular Expressions](https://towardsdatascience.com/easiest-way-to-remember-regular-expressions-regex-178ba518bebd)
"""

df['stars'] = df['star_rating'].str.extract(r'(\d+)').astype(int) # COMPLETE THE CODE
df['stars']

type(df['stars'][0])

df.head()

# COMPLETE CODE HERE
def star_rating_numeric(rating_string):
  return float(rating_string.split()[0])

df['star_rating'] = df['star_rating'].apply(star_rating_numeric)

df.head()

# most coffee shops get 4 or 5 star ratings.
bin_edges = [0.5, 1.5, 2.5, 3.5, 4.5, 5.5]
df['stars'].hist(bins=bin_edges);

"""##2.2 Split into two dataframes based on star rating
good: ratings 4 and 5 <br>
bad: ratings <4
"""

# Complete code here
good = df[df['stars'] >= 4]
bad = df[df['stars'] < 4]

# make sure that all reviews are accounted for in one of the two groups
assert df.shape[0] == good.shape[0] + bad.shape[0]

"""#### Summary of the descriptive token statistics

`word` The specific token that is being analyzed

`appears_in_docs` Number of documents that the word/token appears in

`count` The total number of appearances of that token within the corpus

`rank` Ranking of tokens by count

`fraction_of_total` Fraction of the total tokens that this token makes up

`cumulative_fraction_of_total` Sum of fractional total of ranked tokens, down to and including this token.

`appears_in_fraction_of_docs` Fraction of documents that token appears in
"""

good_wc = count(good["spacy_tokens"])
good_wc.head(10)

bad_wc = count(bad["spacy_tokens"])
bad_wc.head(10)

"""##2.3 Visualize top 20 `good` and top 20 `bad` tokens using a word cloud"""

from wordcloud import WordCloud

# Create a word cloud for good reviews
good_top20_dict = good_wc['fraction_of_total'].to_dict()
good_top20_dict = {str(key): value for key, value in good_top20_dict.items()}

wordcloud = WordCloud(width=800, height=400).generate_from_frequencies(good_top20_dict)
plt.figure(figsize=(10,7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.title("Positive Reviews")
plt.axis('off')
plt.show()

# Create a word cloud for bad reviews
bad_top20_dict = bad_wc['fraction_of_total'].to_dict()
bad_top20_dict = {str(key): value for key, value in bad_top20_dict.items()}

wordcloud = WordCloud(width=800, height=400).generate_from_frequencies(bad_top20_dict)
plt.figure(figsize=(10,7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.title("Negative Reviews")
plt.axis('off')
plt.show()

# COMPLETE CODE HERE
# Good reviews
good_top20 = good_wc[good_wc['rank'] <= 20]
plt.figure(figsize=(10,7))
squarify.plot(sizes=good_top20['fraction_of_total'], label=good_top20['word'], alpha=0.6)
plt.title("Positive Reviews")
plt.axis('off')
plt.show()

# Bad reviews
bad_top20 = bad_wc[bad_wc['rank'] <= 20]
plt.figure(figsize=(10,7))
squarify.plot(sizes=bad_top20['fraction_of_total'], label=bad_top20['word'], alpha=0.6)
plt.title("Negative Reviews")
plt.axis('off')
plt.show()

"""## 3. Find out which words are likely to occur in "good" and "bad" reviews
Ok - let's do a more thorough analysis! <br>
Looking at `df_pos_wc` and `df_neg_wc` we notice that certain words are ranked higher in one data set and lower in the other.
"""

# these are words that are ranked high in one data set but not the other
# you might try working with two keyword lists -- one for the good reviews and one for the bad reviews
key_words = ["love", "not", "delicious", "friendly","great"]

good_wc.head()

df_pos_keywords_mask = good_wc.word.isin(key_words)
df_pos_keywords = good_wc[df_pos_keywords_mask]
df_pos_keywords.head()

df_neg_keywords_mask = bad_wc.word.isin(key_words)
df_neg_keywords = bad_wc[df_neg_keywords_mask]
df_neg_keywords.head()

import seaborn as sns

plt.figure(figsize=(15,6))
plt.title("4 and 5 star reviews: Percent of Documents that Keywords appear in")
sns.barplot(x=df_pos_keywords.word, y=df_pos_keywords.appears_in_fraction_of_docs, palette="rocket", order=key_words);
plt.ylim(0,.42)
plt.show();

plt.figure(figsize=(15,6))
plt.title("1, 2, and 3 star reviews: Percent of Documents that Keywords appear in")
sns.barplot(x=df_neg_keywords.word, y=df_neg_keywords.appears_in_fraction_of_docs, palette="rocket", order=key_words);
plt.ylim(0,.42)
plt.show();